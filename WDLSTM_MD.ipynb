{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d057f",
   "metadata": {
    "id": "c54d057f"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets import WikiText2\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchmetrics import Perplexity\n",
    "import re\n",
    "from torch.nn import Parameter\n",
    "import torch\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e819b6",
   "metadata": {
    "id": "15e819b6"
   },
   "source": [
    "# UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6348566",
   "metadata": {
    "id": "f6348566"
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "class AverageMeter:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch=None, hiddens=None):\n",
    "    model.train()\n",
    "    mi = Perplexity().to(device)\n",
    "    loss_train = AverageMeter()\n",
    "    with tqdm(train_loader, unit='batch') as tepochs:\n",
    "        for x_batch, y_batch in tepochs:\n",
    "            if epoch is not None:\n",
    "                tepochs.set_description(f'epoch:{epoch}')\n",
    "            yp, hiddens = model(x_batch.to(device), hiddens)\n",
    "            loss = loss_fn(yp.transpose(2, 1).to(device), y_batch.to(device))\n",
    "            loss.backward()\n",
    "            hiddens = repackage_hidden(hiddens)\n",
    "            clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            maz = mi(yp, y_batch.to(device))\n",
    "\n",
    "            tepochs.set_postfix(loss=loss_train.avg, pre=mi.compute())\n",
    "            loss_train.update(loss.item())\n",
    "    return model, loss_train.avg, mi.compute().item(), hiddens\n",
    "\n",
    "def evaluate(model, test_loader, loss_fn, hiddens=None):\n",
    "    model.eval()\n",
    "    mi = Perplexity().to(device)\n",
    "    loss_test = AverageMeter()\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in test_loader:\n",
    "            yp, hiddens = model(x_batch.to(device), hiddens)\n",
    "            loss = loss_fn(yp.transpose(2, 1).to(device), y_batch.to(device))\n",
    "            hiddens = repackage_hidden(hiddens)\n",
    "            loss_test.update(loss.item())\n",
    "            maz = mi(yp, y_batch)\n",
    "    print(mi.compute())\n",
    "    return loss_test.avg, mi.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadbfbd7",
   "metadata": {
    "id": "aadbfbd7"
   },
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, dropout=0.5):\n",
    "        if not self.training or not dropout:\n",
    "            return x\n",
    "        m = x.data.new(x.size(0), 1, x.size(2)).bernoulli_(1 - dropout)\n",
    "        mask = m.requires_grad_(False) / (1 - dropout)\n",
    "        mask = mask.expand_as(x)\n",
    "        return mask * x\n",
    "\n",
    "class WeightDrop(torch.nn.Module):\n",
    "    def __init__(self, module, weights, dropout=0, variational=False):\n",
    "        super(WeightDrop, self).__init__()\n",
    "        self.module = module\n",
    "        self.weights = weights\n",
    "        self.dropout = dropout\n",
    "        self.variational = variational\n",
    "        self._setup()\n",
    "\n",
    "    def widget_demagnetizer_y2k_edition(*args, **kwargs):\n",
    "        # We need to replace flatten_parameters with a nothing function\n",
    "        # It must be a function rather than a lambda as otherwise pickling explodes\n",
    "        # We can't write boring code though, so ... WIDGET DEMAGNETIZER Y2K EDITION!\n",
    "        # (╯°□°）╯︵ ┻━┻\n",
    "        return\n",
    "\n",
    "    def _setup(self):\n",
    "        # Terrible temporary solution to an issue regarding compacting weights re: CUDNN RNN\n",
    "        if issubclass(type(self.module), torch.nn.RNNBase):\n",
    "            self.module.flatten_parameters = self.widget_demagnetizer_y2k_edition\n",
    "\n",
    "        for name_w in self.weights:\n",
    "            print('Applying weight drop of {} to {}'.format(self.dropout, name_w))\n",
    "            w = getattr(self.module, name_w)\n",
    "            del self.module._parameters[name_w]\n",
    "            self.module.register_parameter(name_w + '_raw', Parameter(w.data))\n",
    "\n",
    "    def _setweights(self):\n",
    "        for name_w in self.weights:\n",
    "            raw_w = getattr(self.module, name_w + '_raw')\n",
    "            w = None\n",
    "            if self.variational:\n",
    "                mask = torch.autograd.Variable(torch.ones(raw_w.size(0), 1))\n",
    "                if raw_w.is_cuda: mask = mask.cuda()\n",
    "                mask = torch.nn.functional.dropout(mask, p=self.dropout, training=True)\n",
    "                w = mask.expand_as(Parameter(raw_w)) * Parameter(raw_w)\n",
    "            else:\n",
    "                w = torch.nn.functional.dropout(Parameter(raw_w), p=self.dropout, training=self.training)\n",
    "            setattr(self.module, name_w, Parameter(w))\n",
    "\n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        return self.module.forward(*args)\n",
    "\n",
    "def embedded_dropout(embed, words, dropout=0.1, scale=None):\n",
    "    if dropout:\n",
    "        mask = embed.weight.data.new().resize_((embed.weight.size(0), 1)).bernoulli_(1 - dropout).expand_as(embed.weight) / (1 - dropout)\n",
    "        masked_embed_weight = mask * embed.weight\n",
    "    else:\n",
    "        masked_embed_weight = embed.weight\n",
    "    if scale:\n",
    "        masked_embed_weight = scale.expand_as(masked_embed_weight) * masked_embed_weight\n",
    "\n",
    "    padding_idx = embed.padding_idx\n",
    "    if padding_idx is None:\n",
    "        padding_idx = -1\n",
    "\n",
    "    X = torch.nn.functional.embedding(words, masked_embed_weight,\n",
    "    padding_idx, embed.max_norm, embed.norm_type,\n",
    "    embed.scale_grad_by_freq, embed.sparse\n",
    "    )\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6c2ad",
   "metadata": {
    "id": "58d6c2ad"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1230a03c",
   "metadata": {
    "id": "1230a03c"
   },
   "outputs": [],
   "source": [
    "bs = 50\n",
    "seq = 100\n",
    "step = 50\n",
    "rnn_unit = 1200\n",
    "embed_dim = 500\n",
    "hn = None\n",
    "eos = ['eos']\n",
    "dp = 0.3\n",
    "wd_dp = 0.2 \n",
    "dp_h = 0.5\n",
    "# HYPER PARAMETERS ARE NOT SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719cd425",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer('basic_english')\n",
    "train, valid, test = WikiText2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bad8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiSet(Dataset):\n",
    "    \n",
    "    def __init__(self, text, vocab=None):\n",
    "        # cleaning and tokenizing data\n",
    "        # NOT JUST SENDING EVERY SINGLE TRASH TO MODEL WITHOUT ANY CLEANES TO GET LOWER LOSS\n",
    "        tokens = [tokenizer(sentence) + eos for sentence in\n",
    "         ''.join(\n",
    "             [word.lower() for word in \n",
    "                  ''.join([idx for idx in text]) if re.match(\"[A-Za-z0-9.',\\s]\", word)] \n",
    "         ).splitlines()\n",
    "        if len(tokenizer(sentence)) > 20] \n",
    "\n",
    "        # building or getting vocab from input args\n",
    "        if vocab:\n",
    "            self.vocab = vocab  \n",
    "        else:\n",
    "            self.vocab = build_vocab_from_iterator(tokens, min_freq=3) # creating vocab\n",
    "            self.vocab.set_default_index(self.vocab['unk']) # unk tag is set to default\n",
    "\n",
    "        sequences = torch.LongTensor(\n",
    "            [self.vocab[i] for z in tokens for i in z]).unfold(0, seq, step) # SHAPING DATA with torch.unfold()\n",
    "\n",
    "        self.X, self.y = (lambda x: (x[:, :-1], x[:, 1:]))(sequences) # SEPERATING X, y\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, ind):\n",
    "        return self.X[ind], self.y[ind]\n",
    "    \n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        with calling the class you will get vocab\n",
    "        \"\"\"\n",
    "        return self.vocab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ad7e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = WikiSet(train)\n",
    "vocab = train_set()\n",
    "valid_set = WikiSet(valid, vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6296ef47",
   "metadata": {
    "id": "6296ef47"
   },
   "source": [
    "# NEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a4bdbd",
   "metadata": {
    "id": "48a4bdbd"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, bs, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_set, bs, shuffle=1024, drop_last=True) # SETTING SHUFFLE TO SOME SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ee11e",
   "metadata": {
    "id": "b89ee11e"
   },
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_unit, n_layers, n_voc, embeds, dp, wd_dp, dp_h):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_voc, embeds)\n",
    "        self.lstms = nn.ModuleList()\n",
    "        self.dp = dp\n",
    "        self.dropouth = dp_h\n",
    "        self.n_layers = n_layers\n",
    "        self.rnn_unit = rnn_unit\n",
    "        self.embedings = embeds\n",
    "        for l in range(n_layers):\n",
    "            inp = embeds if l==0 else rnn_unit\n",
    "            out = rnn_unit if l != n_layers-1 else embeds\n",
    "            self.lstms.append(WeightDrop(nn.LSTM(inp, out, 1, batch_first=True), ['weight_hh_l0'], wd_dp))\n",
    "        self.fc = nn.Linear(embeds, n_voc)\n",
    "        self.fc.weight = self.embed.weight\n",
    "        self.lockdrop = LockedDropout()\n",
    "    def forward(self, x, hns):\n",
    "        x = self.lockdrop(embedded_dropout(self.embed, x, 0.1), self.dp)\n",
    "        hiddens = list()\n",
    "        for i, layer in enumerate(self.lstms):\n",
    "            x, hn = layer(x, hns[i])\n",
    "            hiddens.append(hn)\n",
    "            if i != self.n_layers - 1:\n",
    "                x = self.lockdrop(x, self.dropouth)\n",
    "        x = self.lockdrop(x, self.dp)\n",
    "        y = self.fc(x)\n",
    "        return y, hiddens\n",
    "\n",
    "\n",
    "    def init_hidden(self, bs):\n",
    "        weight = next(self.parameters()).data\n",
    "\n",
    "        return [(weight.new(1, bs, self.rnn_unit if l != self.n_layers - 1 else torch.tensor((self.embedings))),\n",
    "                weight.new(1, bs, self.rnn_unit if l != self.n_layers - 1 else torch.tensor((self.embedings))))\n",
    "                for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19b894b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a19b894b",
    "outputId": "0772fd23-01f8-4447-e524-3acab244830a"
   },
   "outputs": [],
   "source": [
    "model = MyModel(rnn_unit, n_layers, len(vocab), embed_dim, dp, wd_dp, dp_h).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c976a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "110c976a",
    "outputId": "0c29df5b-4119-4767-fc14-2d2ab8f677a7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d2ffd4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "58d2ffd4",
    "outputId": "20d0579a-9384-4868-cbed-74ea73e804a0"
   },
   "outputs": [],
   "source": [
    "sum([p.numel() for p in model.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103d9e78",
   "metadata": {
    "id": "103d9e78"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=wd, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04e9f98",
   "metadata": {
    "id": "d04e9f98"
   },
   "outputs": [],
   "source": [
    "loss_train_hist = list()\n",
    "loss_valid_hist = list()\n",
    "pre_train_hist = list()\n",
    "pre_valid_hist = list()\n",
    "best_pre_valid = torch.inf\n",
    "hiddens = model.init_hidden(bs)\n",
    "epoch_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79857d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f79857d3",
    "outputId": "c5413341-9337-40b5-f094-131e5c72db0a",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 50\n",
    "for epoch in range(n):\n",
    "    model, train_loss, pre, hiddens = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch, hiddens)\n",
    "    valid_loss, valid_pre = evaluate(model, valid_loader, loss_fn, hiddens)\n",
    "\n",
    "\n",
    "    loss_train_hist.append(train_loss)\n",
    "    loss_valid_hist.append(valid_loss)\n",
    "\n",
    "    pre_train_hist.append(pre)\n",
    "    pre_valid_hist.append(valid_pre)\n",
    "\n",
    "    if valid_pre < best_pre_valid:\n",
    "        torch.save(model,'modelx1.pt')\n",
    "        best_pre_valid =  valid_pre\n",
    "        print('Model SAVED')\n",
    "\n",
    "    epoch_counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29af3b7",
   "metadata": {
    "id": "c29af3b7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(epoch_counter), pre_train_hist, 'r', label='Train')\n",
    "plt.plot(range(epoch_counter), pre_valid_hist, 'g', label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Prp')\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e032f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(epoch_counter), loss_train_hist, 'r', label='Train')\n",
    "plt.plot(range(epoch_counter), loss_valid_hist, 'g', label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Prp')\n",
    "plt.legend()\n",
    "plt.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc405d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiddens = model.init_hidden(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe0e747",
   "metadata": {
    "id": "ebe0e747"
   },
   "outputs": [],
   "source": [
    "first_string = 'Hi i am a language model'\n",
    "input_eval = [vocab[c] for c in first_string]\n",
    "input_eval = torch.LongTensor(input_eval).unsqueeze(dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc6837a",
   "metadata": {
    "id": "7dc6837a"
   },
   "outputs": [],
   "source": [
    "text_generated = ['Hi i am a language model ']\n",
    "for i in range(50):\n",
    "    model.eval()\n",
    "    predictions, hiddens = model(input_eval, hiddens)\n",
    "    predictions = predictions.squeeze() / .7\n",
    "    last_argm = torch.multinomial(F.softmax(predictions, dim=-1), num_samples=1)[-1]\n",
    "    message = torch.cat((input_eval[0], last_argm))[1:]\n",
    "    input_eval = message.unsqueeze(0)\n",
    "    text_generated.append(index2char[last_argm.cpu()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85e7e30",
   "metadata": {
    "id": "d85e7e30"
   },
   "outputs": [],
   "source": [
    "''.join(text_generated).split(eos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0865dec1",
   "metadata": {
    "id": "0865dec1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
